{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5143fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user -U pip setuptools wheel\n",
    "!pip install --user -U nltk swifter spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import swifter\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Platform Dictionary\n",
    "CRYPTO_EXCHANGES = {\n",
    "    'binance': ['binance', 'bnb', 'binance us', 'binance app', 'binance exchange'],\n",
    "    'coinbase': ['coinbase', 'coinbase pro', 'coinbase wallet', 'cb wallet'],\n",
    "    'kraken': ['kraken', 'kraken exchange', 'kraken pro'],\n",
    "    'okx': ['okx', 'okex'],\n",
    "    'kucoin': ['kucoin', 'kucoin exchange'],\n",
    "    'crypto.com': ['crypto.com', 'cro', 'crypto.com app', 'cdc'],\n",
    "    'bybit': ['bybit', 'bybit app']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b93a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRYPTO_FEATURES = {\n",
    "    \"Fees\": [\n",
    "        # General terms\n",
    "        \"fee\", \"fees\", \"trading fee\", \"withdrawal fee\", \"deposit fee\", \"gas fee\",\n",
    "        \"processing fee\", \"platform fee\", \"service fee\", \"commission\", \"charge\",\n",
    "        \"transaction cost\", \"network fee\", \"markup\", \"hidden fee\", \"cost\",\n",
    "        # Sentiment terms\n",
    "        \"expensive\", \"cheap\", \"affordable\", \"overpriced\", \"zero fee\", \"free trading\",\n",
    "        \"fee waiver\", \"transparent pricing\", \"discount\", \"low cost\", \"rebate\",\n",
    "        \"fee hike\", \"savings\", \"fee structure\"\n",
    "    ],\n",
    "    \"User Interface\": [\n",
    "        # General UX/UI terms\n",
    "        \"app\", \"website\", \"ui\", \"ux\", \"interface\", \"design\", \"dashboard\",\n",
    "        \"layout\", \"navigation\", \"update\", \"mobile app\", \"web app\", \"platform\",\n",
    "        \"frontend\", \"web interface\", \"accessibility\",\n",
    "        # Quality and sentiment\n",
    "        \"intuitive\", \"user-friendly\", \"complicated\", \"clunky\", \"responsive\",\n",
    "        \"non-responsive\", \"buggy\", \"glitchy\", \"laggy\", \"fast\", \"slow\",\n",
    "        \"crash\", \"crashes\", \"loading\", \"stuck\", \"freeze\", \"unusable\", \"modern\",\n",
    "        \"clean design\", \"aesthetic\", \"dark mode\", \"theme\", \"customizable\", \"UX issue\",\n",
    "        \"broken button\", \"scroll issue\", \"feature request\"\n",
    "    ],\n",
    "    \"Customer Service\": [\n",
    "        # General terms\n",
    "        \"support\", \"help\", \"customer service\", \"chat\", \"live chat\", \"ticket\",\n",
    "        \"email support\", \"phone support\", \"agent\", \"representative\", \"contact us\",\n",
    "        \"support center\", \"call\", \"support system\", \"contacted\",\n",
    "        # Sentiment & experience\n",
    "        \"unresponsive\", \"no reply\", \"slow response\", \"waiting time\",\n",
    "        \"resolved\", \"unresolved\", \"ignored\", \"escalation\", \"follow-up\",\n",
    "        \"not helpful\", \"friendly support\", \"helpful\", \"rude\", \"professional\",\n",
    "        \"excellent service\", \"terrible support\", \"automated response\", \"human support\",\n",
    "        \"feedback\", \"complaint\", \"issue resolved\", \"reached out\", \"reply delay\"\n",
    "    ],\n",
    "    \"Security\": [\n",
    "        # General terms\n",
    "        \"security\", \"secure\", \"2fa\", \"two-factor\", \"authentication\", \"authorization\",\n",
    "        \"kyc\", \"identity verification\", \"account locked\", \"safety\", \"malware\",\n",
    "        \"cold wallet\", \"hot wallet\", \"security token\", \"encryption\", \"biometric\",\n",
    "        # Threats and issues\n",
    "        \"hack\", \"hacked\", \"breach\", \"scam\", \"fraud\", \"phishing\", \"suspicious activity\",\n",
    "        \"account compromise\", \"withdrawal lock\", \"login issue\", \"security question\",\n",
    "        \"data leak\", \"leaked\", \"exploit\", \"vulnerability\", \"backdoor\", \"DDoS\",\n",
    "        \"locked out\", \"security risk\", \"unauthorized access\", \"suspension\", \"flagged\"\n",
    "    ],\n",
    "    \"Coin Listings\": [\n",
    "        # General listing and support\n",
    "        \"coin\", \"token\", \"asset\", \"cryptocurrency\", \"stablecoin\", \"altcoin\",\n",
    "        \"listing\", \"listed\", \"launched\", \"new coin\", \"added\", \"integrated\", \"supported\",\n",
    "        \"available\", \"pair\", \"market pair\", \"trading pair\", \"support for\",\n",
    "        \"coin support\", \"supported coins\", \"listing announcement\",\n",
    "        # Negative cases\n",
    "        \"delist\", \"delisted\", \"removed\", \"not available\", \"missing coin\",\n",
    "        \"unavailable\", \"coin disappeared\", \"removed token\", \"not listed\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        # System behavior\n",
    "        \"performance\", \"uptime\", \"downtime\", \"crash\", \"crashes\", \"lag\", \"slow\",\n",
    "        \"delay\", \"timeout\", \"error\", \"freeze\", \"stuck\", \"hang\", \"server issue\",\n",
    "        \"connection lost\", \"high latency\", \"unstable\", \"reboot\", \"restart\", \"bug\",\n",
    "        \"real-time\", \"processing\", \"glitch\", \"retry\", \"buffering\", \"loading\",\n",
    "        \"fast\", \"quick\", \"responsive\", \"smooth\", \"snappy\", \"sluggish\", \"laggy\",\n",
    "        # Platform terms\n",
    "        \"server down\", \"platform slow\", \"website not working\", \"issue with app\",\n",
    "        \"maintenance\", \"unexpected error\", \"performance bottleneck\", \"app crash\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19402cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core NLP Components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set processing parameters\n",
    "min_text_length = 5\n",
    "duplicate_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ea6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc Functions\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join tokens back to text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5af947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "        if not text:\n",
    "            return 'neutral', 0.0\n",
    "\n",
    "        scores = sentiment_analyzer.polarity_scores(text)\n",
    "        compound_score = scores['compound']\n",
    "\n",
    "        # Convert score to sentiment category\n",
    "        if compound_score >= 0.05:\n",
    "            sentiment = 'positive'\n",
    "        elif compound_score <= -0.05:\n",
    "            sentiment = 'negative'\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "\n",
    "        return sentiment, compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e88e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    if not text:\n",
    "        return {feature: 0.0 for feature in CRYPTO_FEATURES}\n",
    "\n",
    "    text = text.lower()\n",
    "    feature_scores = {}\n",
    "\n",
    "    for feature, keywords in CRYPTO_FEATURES.items():\n",
    "        feature_phrases = [kw for kw in keywords if kw in text]\n",
    "        feature_text = \"\"\n",
    "\n",
    "        for phrase in feature_phrases:\n",
    "            # Match ~100 character windows around phrase\n",
    "            pattern = r'.{0,100}' + re.escape(phrase) + r'.{0,100}'\n",
    "            matches = re.findall(pattern, text)\n",
    "            feature_text += ' '.join(matches) + ' '\n",
    "\n",
    "        if feature_text:\n",
    "            _, sentiment_score = analyze_sentiment(feature_text)\n",
    "            feature_scores[feature] = sentiment_score\n",
    "        else:\n",
    "            feature_scores[feature] = 0.0\n",
    "\n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c39207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    if not text or len(text) < 20:\n",
    "        return []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        return [{'text': ent.text, 'type': ent.label_} for ent in doc.ents]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tfidf_row, feature_names, top_n=10):\n",
    "    try:\n",
    "        scores = tfidf_row.toarray().flatten()\n",
    "        top_indices = scores.argsort()[::-1][:top_n]\n",
    "        return [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting keywords: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe387acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas() \n",
    "def preprocess(df):\n",
    "    print(\"Starting comprehensive preprocessing...\")\n",
    "\n",
    "    # Clean text\n",
    "    if 'Text' in df.columns:\n",
    "        print(\"Cleaning text...\")\n",
    "        df['Cleaned Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "    # Filter short texts\n",
    "    print(\"Filtering out short texts...\")\n",
    "    df = df[df['Cleaned Text'].str.split().str.len() > min_text_length].copy()\n",
    "\n",
    "    # Sentiment analysis\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    sentiment_results = df['Cleaned Text'].apply(analyze_sentiment)\n",
    "    df['Sentiment'] = sentiment_results.apply(lambda x: x[0])\n",
    "    df['Sentiment Score'] = sentiment_results.apply(lambda x: x[1])\n",
    "\n",
    "    # Feature scores\n",
    "    print(\"Extracting feature scores...\")\n",
    "    feature_scores = df['Cleaned Text'].swifter.apply(extract_features)\n",
    "    for feature in CRYPTO_FEATURES.keys():\n",
    "        df[feature] = feature_scores.apply(lambda x: x.get(feature, 0.0))\n",
    "\n",
    "    # Extract keywords using TF-IDF (fit once)\n",
    "    print(\"Fitting TF-IDF vectorizer...\")\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=100)\n",
    "    X = vectorizer.fit_transform(df['Cleaned Text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Extracting entities with progress bar...\")\n",
    "    df['Entities'] = df['Cleaned Text'].progress_apply(lambda x: json.dumps(extract_entities(x)))\n",
    "    \n",
    "    print(\"Extracting keywords with progress bar...\")\n",
    "    df['Keywords'] = [json.dumps(extract_keywords(X[i], feature_names)) for i in tqdm(range(X.shape[0]))]\n",
    "\n",
    "\n",
    "    print(f\"Preprocessing complete. {len(df)} records remain.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df, text_column='Cleaned Text', threshold=None):\n",
    "        threshold = threshold or duplicate_threshold\n",
    "        print(f\"Detecting near-duplicate content with threshold {threshold}...\")\n",
    "\n",
    "        # Get non-empty texts\n",
    "        texts = df[text_column].dropna().tolist()\n",
    "        if len(texts) < 2:\n",
    "            return df\n",
    "\n",
    "        # Create TF-IDF matrix\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Calculate pairwise similarity\n",
    "        duplicate_indices = set()\n",
    "\n",
    "        # For large datasets, process in batches to avoid memory issues\n",
    "        batch_size = 1000\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_end = min(i + batch_size, len(texts))\n",
    "            batch_matrix = tfidf_matrix[i:batch_end]\n",
    "\n",
    "            # Calculate cosine similarity between this batch and all documents\n",
    "            similarities = cosine_similarity(batch_matrix, tfidf_matrix)\n",
    "\n",
    "            # Find duplicates\n",
    "            for batch_idx, sim_scores in enumerate(similarities):\n",
    "                doc_idx = i + batch_idx\n",
    "                # Find similar documents (excluding self-comparison)\n",
    "                similar_indices = np.where(sim_scores > threshold)[0]\n",
    "\n",
    "                for similar_idx in similar_indices:\n",
    "                    if similar_idx != doc_idx and similar_idx > doc_idx:\n",
    "                        # Keep the document with more content or higher engagement\n",
    "                        if len(texts[doc_idx]) < len(texts[similar_idx]):\n",
    "                            duplicate_indices.add(doc_idx)\n",
    "                        else:\n",
    "                            duplicate_indices.add(similar_idx)\n",
    "\n",
    "        # Create a duplicate flag\n",
    "        df['is_duplicate'] = df.index.isin(duplicate_indices)\n",
    "\n",
    "        # Filter out duplicates\n",
    "        df_no_duplicates = df[~df['is_duplicate']]\n",
    "\n",
    "        print(f\"Removed {len(duplicate_indices)} duplicate records. {len(df_no_duplicates)} records remaining.\")\n",
    "        return df_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('crypto_exchange_data_raw.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (includes cleaning, sentiment analysis, etc.)\n",
    "processed_df = preprocess(df)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ef1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "deduplicated_df = detect_duplicates(processed_df)\n",
    "deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = deduplicated_df.fillna('')  # to avoid nulls when exporting to solr\n",
    "\n",
    "deduplicated_df['ID'] = deduplicated_df.index.astype(str) # solr needs id column in str not int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96475586",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df.to_csv('../data/crypto_exchange_data_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
