{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5143fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user -U pip setuptools wheel\n",
    "!pip install --user -U nltk swifter spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import swifter\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3246848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Platform Dictionary\n",
    "CRYPTO_EXCHANGES = {\n",
    "    'binance': ['binance', 'bnb', 'binance us', 'binance app', 'binance exchange'],\n",
    "    'coinbase': ['coinbase', 'coinbase pro', 'coinbase wallet', 'cb wallet'],\n",
    "    'kraken': ['kraken', 'kraken exchange', 'kraken pro'],\n",
    "    'okx': ['okx', 'okex'],\n",
    "    'kucoin': ['kucoin', 'kucoin exchange'],\n",
    "    'crypto.com': ['crypto.com', 'cro', 'crypto.com app', 'cdc'],\n",
    "    'bybit': ['bybit', 'bybit app']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b93a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Feature Lexicons\n",
    "CRYPTO_FEATURES = {\n",
    "    'Fees': [\n",
    "        'fee', 'fees', 'trading fee', 'withdrawal fee', 'deposit fee',\n",
    "        'low fee', 'high fee', 'expensive', 'cheap', 'commission',\n",
    "        'hidden fee', 'transparent pricing', 'zero fee', 'cost', 'charge',\n",
    "        'markup', 'processing fee', 'transaction cost', 'gas fee'\n",
    "    ],\n",
    "    'User Interface': [\n",
    "        'app', 'website', 'ui', 'ux', 'interface', 'design', 'layout',\n",
    "        'navigation', 'bug', 'glitch', 'slow', 'fast', 'responsive',\n",
    "        'usability', 'mobile app', 'dashboard', 'update', 'dark mode',\n",
    "        'intuitive', 'user-friendly', 'complicated', 'laggy', 'crashes',\n",
    "        'loading', 'experience', 'accessibility'\n",
    "    ],\n",
    "    'Customer Service': [\n",
    "        'support', 'help', 'customer', 'service', 'response', 'ticket',\n",
    "        'chat', 'email', 'reply', 'unresponsive', 'delay', 'resolved',\n",
    "        'complaint', 'agent', 'representative', 'live chat', 'call',\n",
    "        'waiting time', 'inquiry', 'feedback', 'escalation', 'not helpful',\n",
    "        'ignored', 'follow-up'\n",
    "    ],\n",
    "    'Security': [\n",
    "        'secure', 'security', 'hack', 'breach', 'phishing', '2fa',\n",
    "        'safety', 'account locked', 'withdrawal lock', 'verify',\n",
    "        'verification', 'suspicious activity', 'identity theft', 'scam',\n",
    "        'fraud', 'authentication', 'kyc', 'malware', 'cold wallet',\n",
    "        'hot wallet', 'security token', 'ddos', 'data leak'\n",
    "    ],\n",
    "    'Coin Listings': [\n",
    "        'listed', 'coin', 'token', 'listing', 'altcoin', 'available',\n",
    "        'supported', 'delist', 'new coin', 'available pairs',\n",
    "        'cryptocurrency', 'asset', 'stablecoin', 'pairing', 'market pair',\n",
    "        'trading pair', 'not available', 'support for', 'removed',\n",
    "        'launched', 'integrated'\n",
    "    ],\n",
    "    'Performance': [\n",
    "        'crash', 'slow', 'lag', 'error', 'fail', 'stable', 'reliable',\n",
    "        'outage', 'downtime', 'performance', 'uptime', 'maintenance',\n",
    "        'server issue', 'connection lost', 'timeout', 'freeze', 'buggy',\n",
    "        'high latency', 'stuck', 'reboot', 'real-time', 'speed'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19402cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core NLP Components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set processing parameters\n",
    "min_text_length = 5\n",
    "duplicate_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ea6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc Functions\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join tokens back to text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5af947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "        if not text:\n",
    "            return 'neutral', 0.0\n",
    "\n",
    "        scores = sentiment_analyzer.polarity_scores(text)\n",
    "        compound_score = scores['compound']\n",
    "\n",
    "        # Convert score to sentiment category\n",
    "        if compound_score >= 0.05:\n",
    "            sentiment = 'positive'\n",
    "        elif compound_score <= -0.05:\n",
    "            sentiment = 'negative'\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "\n",
    "        return sentiment, compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e88e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    if not text:\n",
    "        return {feature: 0.0 for feature in CRYPTO_FEATURES}\n",
    "\n",
    "    text = text.lower()\n",
    "    feature_scores = {}\n",
    "\n",
    "    for feature, keywords in CRYPTO_FEATURES.items():\n",
    "        feature_phrases = [kw for kw in keywords if kw in text]\n",
    "        feature_text = \"\"\n",
    "\n",
    "        for phrase in feature_phrases:\n",
    "            # Match ~100 character windows around phrase\n",
    "            pattern = r'.{0,100}' + re.escape(phrase) + r'.{0,100}'\n",
    "            matches = re.findall(pattern, text)\n",
    "            feature_text += ' '.join(matches) + ' '\n",
    "\n",
    "        if feature_text:\n",
    "            _, sentiment_score = analyze_sentiment(feature_text)\n",
    "            feature_scores[feature] = sentiment_score\n",
    "        else:\n",
    "            feature_scores[feature] = 0.0\n",
    "\n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c39207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    if not text or len(text) < 20:\n",
    "        return []\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        return [{'text': ent.text, 'type': ent.label_} for ent in doc.ents]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tfidf_row, feature_names, top_n=10):\n",
    "    try:\n",
    "        scores = tfidf_row.toarray().flatten()\n",
    "        top_indices = scores.argsort()[::-1][:top_n]\n",
    "        return [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting keywords: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe387acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas() \n",
    "def preprocess(df):\n",
    "    print(\"Starting comprehensive preprocessing...\")\n",
    "\n",
    "    # Clean text\n",
    "    if 'Text' in df.columns:\n",
    "        print(\"Cleaning text...\")\n",
    "        df['Cleaned Text'] = df['Text'].apply(clean_text)\n",
    "\n",
    "    # Filter short texts\n",
    "    print(\"Filtering out short texts...\")\n",
    "    df = df[df['Cleaned Text'].str.split().str.len() > min_text_length].copy()\n",
    "\n",
    "    # Sentiment analysis\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    sentiment_results = df['Cleaned Text'].apply(analyze_sentiment)\n",
    "    df['Sentiment'] = sentiment_results.apply(lambda x: x[0])\n",
    "    df['Sentiment Score'] = sentiment_results.apply(lambda x: x[1])\n",
    "\n",
    "    # Feature scores\n",
    "    print(\"Extracting feature scores...\")\n",
    "    feature_scores = df['Cleaned Text'].swifter.apply(extract_features)\n",
    "    for feature in CRYPTO_FEATURES.keys():\n",
    "        df[feature] = feature_scores.apply(lambda x: x.get(feature, 0.0))\n",
    "\n",
    "    # Extract keywords using TF-IDF (fit once)\n",
    "    print(\"Fitting TF-IDF vectorizer...\")\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=100)\n",
    "    X = vectorizer.fit_transform(df['Cleaned Text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(\"Extracting entities with progress bar...\")\n",
    "    df['Entities'] = df['Cleaned Text'].progress_apply(lambda x: json.dumps(extract_entities(x)))\n",
    "    \n",
    "    print(\"Extracting keywords with progress bar...\")\n",
    "    df['Keywords'] = [json.dumps(extract_keywords(X[i], feature_names)) for i in tqdm(range(X.shape[0]))]\n",
    "\n",
    "\n",
    "    print(f\"Preprocessing complete. {len(df)} records remain.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df, text_column='Cleaned Text', threshold=None):\n",
    "        threshold = threshold or duplicate_threshold\n",
    "        print(f\"Detecting near-duplicate content with threshold {threshold}...\")\n",
    "\n",
    "        # Get non-empty texts\n",
    "        texts = df[text_column].dropna().tolist()\n",
    "        if len(texts) < 2:\n",
    "            return df\n",
    "\n",
    "        # Create TF-IDF matrix\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Calculate pairwise similarity\n",
    "        duplicate_indices = set()\n",
    "\n",
    "        # For large datasets, process in batches to avoid memory issues\n",
    "        batch_size = 1000\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_end = min(i + batch_size, len(texts))\n",
    "            batch_matrix = tfidf_matrix[i:batch_end]\n",
    "\n",
    "            # Calculate cosine similarity between this batch and all documents\n",
    "            similarities = cosine_similarity(batch_matrix, tfidf_matrix)\n",
    "\n",
    "            # Find duplicates\n",
    "            for batch_idx, sim_scores in enumerate(similarities):\n",
    "                doc_idx = i + batch_idx\n",
    "                # Find similar documents (excluding self-comparison)\n",
    "                similar_indices = np.where(sim_scores > threshold)[0]\n",
    "\n",
    "                for similar_idx in similar_indices:\n",
    "                    if similar_idx != doc_idx and similar_idx > doc_idx:\n",
    "                        # Keep the document with more content or higher engagement\n",
    "                        if len(texts[doc_idx]) < len(texts[similar_idx]):\n",
    "                            duplicate_indices.add(doc_idx)\n",
    "                        else:\n",
    "                            duplicate_indices.add(similar_idx)\n",
    "\n",
    "        # Create a duplicate flag\n",
    "        df['is_duplicate'] = df.index.isin(duplicate_indices)\n",
    "\n",
    "        # Filter out duplicates\n",
    "        df_no_duplicates = df[~df['is_duplicate']]\n",
    "\n",
    "        print(f\"Removed {len(duplicate_indices)} duplicate records. {len(df_no_duplicates)} records remaining.\")\n",
    "        return df_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3323c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('crypto_exchange_data_raw.csv')\n",
    "\n",
    "# Use Vader Sentiment Analysis instead of TextBlob's\n",
    "df = df.drop(['Sentiment'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data (includes cleaning, sentiment analysis, etc.)\n",
    "processed_df = preprocess(df)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ef1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "deduplicated_df = detect_duplicates(processed_df)\n",
    "deduplicated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = deduplicated_df.fillna('')  # to avoid nulls when exporting to solr\n",
    "\n",
    "deduplicated_df['ID'] = deduplicated_df.index.astype(str) # solr needs id column in str not int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96475586",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df.to_csv('../data/crypto_exchange_data_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
